---
title: "Practical Machine Learning Project"
author: "Joseph Chou"
date: "October 20, 2015"
output: html_document
---

### Introduction

Data from accelerometers on the belt, forearm, arm, and dumbell were collected from 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways (information on the Weight Lifting Exercise Dataset at http://groupware.les.inf.puc-rio.br/har). The goal is to develope an algorithm to predict the way in which participants did the exercise (the "classe" variable in the training set).

### Initialization

```{r initialization, warning = FALSE, message = FALSE, cache = TRUE}
setwd("~/Desktop/08 Practical machine learning/project")

library(plyr)
library(randomForest)
library(caret)

training <- read.csv('pml-training.csv', stringsAsFactors = TRUE)
```

### Data cleaning

The training data includes 19,622 observations with the outcome variable ("classe") and an additional 159 possible predictors. Further inspection showed that many possible predictors were mostly empty, consisting of empty strings or NA. These information-scarce predictors were removed. In addition, the first 7 columns included variables indices, timestamps, and usernames, and were also removed, resulting in a final training set of 19,622 observations on 53 variables (including the outcome).

```{r data_preparation, warning = FALSE, message = FALSE, cache = TRUE}
t <- training
t <- t[ , -which(sapply( t, function(x) { sum(is.na(x))}) > 19000)] # remove when majority are NA
t <- t[ , -which(sapply( t, function(x) { sum(x == "") }) > 19000)] # remove when majority are ""
t <- t[ , -c(1:7)] # remove X, user_name, 3 time stamps, new_window, and num_window
```

### Overall Study Design

Outline of study design

* error rate: defined as the percent accuracy of the "classe" prediction
* split data into training and testing sets
* choose features using cross-validation, on the training set
* choose model using cross-validation, on the training set
* apply prediction model to testing set, to estimate out of sample error

The data was splitto reserve 75% of the data ('train') for model selection and generation and 25% of the data ('test') for final model validation, to allow estimation of out of sample error on a data subset never used for model generation.

```{r data_split, warning = FALSE, message = FALSE, cache = TRUE}

set.seed(2357)
in_train <- createDataPartition( y = training$classe, p = 0.75, list = FALSE)
train <- t[ in_train, ]
test <- t[ -in_train, ]
```

### Model selection

Five-fold cross validation was used to compare four different classification algorithms for model selection:

* rpart: classification tree
* lda: linear discriminant analysis
* gbm: boosted trees
* rf: random forests

Each of the 4 models was separately trained on k=5 different folds of the data and the average prediction accuracy for each model compared.

```{r cross_validation_methods, warning = FALSE, message = FALSE, cache = TRUE}
k <- 5 # k-fold cross-validation
folds <- createFolds(y = train$classe, k = k, list = TRUE, returnTrain = TRUE)

methods = c("rpart", "lda", "gbm", "rf") # methods to compare via k-fold cross-validation
acc_df <- data.frame( a = numeric(k) ) # probably a better way exists, but need to pre-allocate rows frame

for (method in methods) { # iterate through each method to test
  accuracy <- c() # vector of prediction accuracy, will be length k
  for (fold in folds) { # iterate through each of k folds
    # caret might have a better way, but this allows different model parameters
    if ( method == "gbm") { # verbose = FALSE to reduce output spam
      model <- train( classe ~ ., data = train[fold,], method = method, verbose = FALSE)
    } else if ( method == "rf" ) { # may be faster than using caret
      randomForest( classe ~ ., data = train[fold,], importance = TRUE)
    } else { # these seem fine with caret
      model <- train( classe ~ ., data = train[fold,], method = method) # works for rpart and lda
    }
    pred <- predict(model, train[-fold, ])
    accuracy <- c(accuracy, mean(pred == train[-fold, "classe"])) # build vector of prediction accuracies
  }
  acc_df[, method] <- accuracy # add new column of accuracies to dataframe of accuracies
}
acc_df[ , 'a'] <- NULL # remove the initial placeholder column
sapply(acc_df, function(x) { round(mean(x),3) }) # display mean prediction accuracy for each method
```

The random forest classifier had the best average prediction accuracy of 97.2%, followed by boosted trees ('gbm') at 95.9%, with the other two classifiers with significantly worse performance. Because the performance seems quite good, and because we don't have an odd number of good models (to allow easy combination of predictors and majority vote), we will elect to use the random forest algorithm for final model creation.

### Final model generation

Using k-fold cross-validation comparison of models allowed a better estimate of prediction accuracy than simply comparing the training set errors if the entire training set were used. However, now that a model has been selected, we will create the final model using the entire 75% training set.

```{r random_forest, warning = FALSE, message = FALSE, cache = TRUE}
model_rf <- randomForest( classe ~ ., data = train, importance = TRUE)
```

With random forests, we can assess the order of importance of predictor variables. As can be seen, a subset of the 52 predictors may be sufficient -- in the future, if there were concerns for overfitting resulting in worsened test error, we could repeat k-fold cross validation to select the optimal variables to maximize performance.

```{r rf_variable_plot, warning = FALSE, message = FALSE, cache = TRUE}
varImpPlot(model_rf)
```

### Prediction of out of sample performance

Finally, using the 25% of the original dataset which has never been used for model selection, we can obtain our best available assessment of out of sample test error, as a prediction of how our model will perform on new measurements.

```{r rf_assessment, warning = FALSE, message = FALSE, cache = TRUE}
pred <- predict( model_rf, test)
confusionMatrix(pred, test$classe)
saveRDS(model_rf, "model_rf.rds") # allow reloading the model to predict on future data
# model_rf <- readRDS("model_rf.rds") # command to reload the model
```

Our final model has an estimated out of sample prediction accuracy of **99.6%**.

We can also predict on the small file of 20 observations, for separate submission.

```{r rf_twenty, warning = FALSE, message = FALSE, cache = TRUE}
test_cases <- read.csv('pml-testing.csv', stringsAsFactors = TRUE)
predict(model_rf, test_cases)
```

